cls <- 463
wordcloud(word[class==cls], freq[class==cls])
wordcloud(word[class==cls], 1/freq[class==cls])
cls <- 464
wordcloud(word[class==cls], 1/freq[class==cls])
size <- 5000
common <- alldat[rev(order(freq))[64:(63+size)],]
rare <- alldat[order(freq)[1:size],]
sampled <- alldat[sample(1:nrow(alldat), size),]
print( common$word )
size <- 6000
common <- alldat[rev(order(freq))[64:(63+size)],]
rare <- alldat[order(freq)[1:size],]
sampled <- alldat[sample(1:nrow(alldat), size),]
print( common$word )
print(rare$word)
hist(common$class, col='grey', breaks=500)
subdat <- common
search()
installed.packages()
rownames(installed.packages())
"h2o" %in% rownames(installed.packages())
install.packages("h2o")
install.packages("h2o")
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
install.packages("h2o", repos=(c("http://s3.amazonaws.com/h2o-release/h2o/rel-kahan/5/R", getOption("repos"))))
install.packages("h2o")
library(h2o)
localH2O = h2o.init()
demo(h2o.glm)
par()
par(mai=rep(1,4))
demo(h2o.glm)
y <- apply(iris[,1:4],2,sum)
y
iris.hex <-  h2o.uploadFile(localH2O, path = system.file("extdata", "iris.csv", package="h2o"), key = "iris.hex")
summary(iris.hex)
library(h2o)                # Load H2O library
localH2O = h2o.init()       # initial H2O locl instance
# Upload iris file from the H2O package into the H2O local instance
iris.hex <-  h2o.uploadFile(localH2O, path = system.file("extdata", "iris.csv", package="h2o"), key = "iris.hex")
summary(iris.hex)
iris.hex <-  h2o.uploadFile(localH2O, path = system.file("extdata", "iris.csv", package="h2o"), key = "iris.hex")
?system.file
getClusterSizes()
install.packages("RSpark")
install.packages("SparkR")
library(SparkR)
shiny::runApp('Blog/ec2/201508wikiwords/wikiwords-1.0')
?uninstall.packages
?install.packages
library("tsne", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
detach("package:utils", unload=TRUE)
detach("package:stats", unload=TRUE)
detach("package:methods", unload=TRUE)
detach("package:grDevices", unload=TRUE)
detach("package:graphics", unload=TRUE)
detach("package:datasets", unload=TRUE)
install.packages(c("base64enc", "boot", "class", "cluster", "codetools", "curl", "devtools", "doParallel", "dplyr", "evaluate", "fields", "foreach", "foreign", "formatR", "git2r", "h2o", "highr", "httpuv", "irlba", "iterators", "jsonlite", "KernSmooth", "lattice", "manipulate", "mapproj", "maps", "maptools", "MASS", "Matrix", "mgcv", "mime", "nlme", "nnet", "packrat", "PKI", "quantmod", "R6", "Rcpp", "roxygen2", "rpart", "scales", "shiny", "sp", "spam", "spatial", "statmod", "stringi", "survival", "TDA", "xml2"))
shiny::runApp('Blog/ec2/201508wikiwords/wikiwords-1.0')
library(igraph)
?"igraph"
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
install.packages("Rstan")
install.packages("RStan")
install.packages("rstan")
library(rstan)
?"rstan"
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
?fluidRow
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
?sidebarPanel
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
library(shiny)
?addPopover
?"shiny"
library(shinyBS)
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
shiny::runApp('Blog/ec2/_INPROGRESS_201512netflow/netflow-2.0')
?shinyBS
?shinyBS
shiny::runApp('Blog/ec2/201512ttest/ttest')
shiny::runApp('Blog/ec2/201512ttest/ttest')
shiny::runApp('Blog/ec2/201512ttest/ttest')
shiny::runApp('Blog/ec2/201512ttest/ttest')
shiny::runApp('Blog/ec2/201512nfgraph/nfgraph')
shiny::runApp('Blog/ec2/201512nfgraph/nfgraph')
shiny::runApp('Blog/ec2/201512nfgraph/nfgraph')
shiny::runApp('Blog/ec2/201512nfgraph/nfgraph')
library(igraph)
?"igraph"
addTrans <- function(color,trans)
{
# This function adds transparancy to a color.
# Define transparancy with an integer between 0 and 255
# 0 being fully transparant and 255 being fully visable
# Works with either color and trans a vector of equal length,
# or one of the two of length 1.
if (length(color)!=length(trans)&!any(c(length(color),length(trans))==1)) stop("Vector lengths not correct")
if (length(color)==1 & length(trans)>1) color <- rep(color,length(trans))
if (length(trans)==1 & length(color)>1) trans <- rep(trans,length(color))
num2hex <- function(x)
{
hex <- unlist(strsplit("0123456789ABCDEF",split=""))
return(paste(hex[(x-x%%16)/16+1],hex[x%%16+1],sep=""))
}
rgb <- rbind(col2rgb(color),trans)
res <- paste("#",apply(apply(rgb,2,num2hex),2,paste,collapse=""),sep="")
return(res)
}
# generate data:
make.data <- function(n1, n2, mu1, mu2, sd1, sd2){
tmp1 <- cbind(
rnorm(n1, mean=mu1[1], sd=sd1[1]),
rnorm(n1, mean=mu1[2], sd=sd1[2]),
rep(0, n1)
)
tmp2 <- cbind(
rnorm(n2, mean=mu2[1], sd=sd2[1]),
rnorm(n2, mean=mu2[2], sd=sd2[2]),
rep(1, n2)
)
out <- data.frame( rbind(tmp1,tmp2) )
names(out) <- c("x1","x2","y")
out
}
##########################################################
n1 <- 1000
n2 <- 30
mu1 <- c(-1,-1); sd1 <- c(1,1)
mu2 <- c(1,1); sd2 <- c(1,1)
train <- make.data(n1, n2, mu1, mu2, sd1, sd2)
plot(train[,1:2], col=addTrans(train$y+2, 150), pch=19, xlab="", ylab="")
# perform logistic regression:
model <- glm(y ~ x1 + x2, family=binomial(), data=train)
coef <- model$coefficients
abline(-coef[1]/coef[3], -coef[2]/coef[3])
summary(model)
# suppose contingency table:
#
#           predicted
#           0   1
# actual 0  a   b
#        1  c   d
#
# Then: FPR       = b/(a+b)   TPR    = d/(c+d)
#       precision = d/(b+d)   recall = d/(c+d)
#
# build ROC curve
# FPR = fraction of the randoms that are scored as causal
# TPR = fraction of the causals that are scored as causal
test <- make.data(100, 100, mu1, mu2, sd1, sd2)
pred <- cbind( predict(model, type="response", newdata=test), test$y )
idx <- seq(0,1,0.01)
roc <- do.call( rbind,
lapply(idx, function(thresh){
n <- nrow(pred)
cond <- (pred[,1] >= thresh)
fpr <- sum(pred[cond,2]==0)/sum(pred[,2]==0)
tpr <- sum(pred[cond,2]==1)/sum(pred[,2]==1)
c(fpr, tpr)
})
)
plot(roc, type='b', col='red', pch=19, cex=0.5,
xlab="False positive rate", ylab="True positive rate",
frame.plot=0)
segments(0,0,1,1, col='grey')
#plot(test[,1:2], col=addTrans(test$y+2, 150), pch=19, xlab="", ylab="")
#abline(-coef[1]/coef[3], -coef[2]/coef[3])
# build PR curve
# precision = fraction of those scored as causal that are correct
# recall fraction of the causal that are scored as causal
prerec <- do.call( rbind,
lapply(idx, function(thresh){
n <- nrow(pred)
cond <- (pred[,1] >= thresh)
precision <- sum(pred[cond,2]==1)/sum(cond)
recall <- sum(pred[cond,2]==1)/sum(pred[,2]==1)
c(precision, recall)
})
)
plot(prerec, type='b', col='red', pch=19, cex=0.5,
xlab="Recall", ylab="Precision",
frame.plot=0)
library(RSQLite)
library(dplyr)
path <- "~/Kaggle/nips2015"
db <- dbConnect(dbDriver("SQLite"), sprintf("%s/database.sqlite", path))
tables <- dbGetQuery(db, "SELECT Name FROM sqlite_master WHERE type='table'")
colnames(tables) <- c("Name")
tables <- tables %>%
rowwise() %>%
mutate(RowCount=dbGetQuery(db, paste0("SELECT COUNT(Id) RowCount FROM ", Name))$RowCount[1])
print.table(tables)
tables
plookup <- dbGetQuery(db, "SELECT * FROM Papers")
names(plookup)
head(plookup[,1:3])
# paper-author correspondence:
paper_authors <- dbGetQuery(db, "SELECT * FROM PaperAuthors")
head(paper_authors)
# papers:
papers <- aggregate(paper_authors, by = list(paper_authors$PaperId), FUN=c)[,c(1,4)]
names(papers) <- c("PaperId", "AuthorId")
# authors:
authors <- aggregate(paper_authors, by = list(raw$AuthorId), FUN=c)[,c(1,3)]
names(authors) <- rev(names(papers))
###################################################################
# paper count discrepancy
p401 <- papers[,1]
p403 <- plookup[,1]
p403[!(p403 %in% p401)]
# so papers 5666 and 5745 are not in the PaperAuthors table
# paper id-to-details function for later use:
pfind <- function(x){ plookup[which(plookup[,1] %in% x), 2:3] }
# author id-to-details function for later use:
afind <- function(x){ alookup[which(alookup[,1] %in% x), 2] }
pfind(5666)
pfind(5745)
# Examining the pdfs folder, it seems these two papers have been mis-labelled
# in the Kaggle data set.
###################################################################
# first look...
# numbers of authors per paper:
nauth <- sapply(papers$AuthorId, length)
table(nauth)
# which papers have many authors?
N <- 10
p <- papers[nauth==N, ]$PaperId
pfind(p)
# numbers of papers per author:
npap <- sapply(authors$PaperId, length)
table(npap)
# who has many papers?
N <- 7
a <- authors[npap==N, ]$AuthorId
afind(a)
# what are the titles of these papers?
idx <- 1
# i.e. which author in the above list if more than one
aid <- a[idx]
pid <- papers[sapply(papers$AuthorId, function(a) aid %in% a),]$PaperId
function (x, table, nomatch = NA_integer_, incomparables = NULL)
.Internal(match(x, table, nomatch, incomparables))
plookup[plookup$Id %in% pid, 2:3]
N <- 7
a <- authors[npap==N, ]$AuthorId
authors <- aggregate(paper_authors, by = list(raw$AuthorId), FUN=c)[,c(1,3)]
aggregate(paper_authors, by = list(raw$AuthorId), FUN=c)
paper_authors
authors <- aggregate(paper_authors, by = list(paper_authors$AuthorId), FUN=c)[,c(1,3)]
names(authors) <- rev(names(papers))
###################################################################
# paper count discrepancy
p401 <- papers[,1]
p403 <- plookup[,1]
p403[!(p403 %in% p401)]
# so papers 5666 and 5745 are not in the PaperAuthors table
# paper id-to-details function for later use:
pfind <- function(x){ plookup[which(plookup[,1] %in% x), 2:3] }
# author id-to-details function for later use:
afind <- function(x){ alookup[which(alookup[,1] %in% x), 2] }
pfind(5666)
pfind(5745)
# Examining the pdfs folder, it seems these two papers have been mis-labelled
# in the Kaggle data set.
###################################################################
# first look...
# numbers of authors per paper:
nauth <- sapply(papers$AuthorId, length)
table(nauth)
# which papers have many authors?
N <- 10
p <- papers[nauth==N, ]$PaperId
pfind(p)
# numbers of papers per author:
npap <- sapply(authors$PaperId, length)
table(npap)
# who has many papers?
N <- 7
a <- authors[npap==N, ]$AuthorId
afind(a)
# what are the titles of these papers?
idx <- 1
# i.e. which author in the above list if more than one
aid <- a[idx]
pid <- papers[sapply(papers$AuthorId, function(a) aid %in% a),]$PaperId
plookup[plookup$Id %in% pid, 2:3]
###################################################################
# author-author edges of paper co-authorship:
na <- nrow(authors)
coauthor_edges <- matrix(nrow=0, ncol=3)
for(i in 1:(na - 1))
for(j in (i+1):na){
wt <- length(intersect(authors[i,2][[1]], authors[j,2][[1]]))
if(wt > 0){
#cat(wt, " ")
coauthor_edges <- rbind(coauthor_edges, c(authors[i,1], authors[j,1] ,wt))
}
}
# collaborations on more than paper?
table(coauthor_edges[,3])
# write to file:
afile <- sprintf("%s/coauthor_edges.txt", path)
write.table(coauthor_edges, file=afile, row.names=FALSE, col.names=FALSE)
# the co-author graph:
library(igraph)
ag <- read.graph(pipe(sprintf("cat %s | cut -d',' -f1,2", afile)), format="ncol", directed=FALSE)
V(ag)$name <- sapply(V(ag)$name, function(x) as.character( afind(x)[1][[1]] ))
# note g has 1066 vertices out of the total 1073 authors
# degree distribution:
table(degree(ag))
# connected components:
cc <- clusters(ag)
table(cc$csize)
# view the full graph:
V(ag)$size <- log(1 + degree(ag))
V(ag)$color <- "white"
V(ag)$frame.color <- "white"
V(ag)$label <- V(ag)$name
V(ag)$label.cex <- 0.2
plot(ag)
alookup <- dbGetQuery(db, "SELECT * FROM Authors")
head(alookup)
# paper details:
plookup <- dbGetQuery(db, "SELECT * FROM Papers")
names(plookup)
head(plookup[,1:3])
# paper-author correspondence:
paper_authors <- dbGetQuery(db, "SELECT * FROM PaperAuthors")
head(paper_authors)
# papers:
ag <- read.graph(pipe(sprintf("cat %s | cut -d',' -f1,2", afile)), format="ncol", directed=FALSE)
V(ag)$name <- sapply(V(ag)$name, function(x) as.character( afind(x)[1][[1]] ))
# note g has 1066 vertices out of the total 1073 authors
# degree distribution:
table(degree(ag))
# connected components:
cc <- clusters(ag)
table(cc$csize)
# view the full graph:
V(ag)$size <- log(1 + degree(ag))
V(ag)$color <- "white"
V(ag)$frame.color <- "white"
V(ag)$label <- V(ag)$name
V(ag)$label.cex <- 0.2
plot(ag)
# view the larger components:
threshold <- 9
idx <- which(cc$csize > threshold)
big <- induced.subgraph(ag, V(ag)[cc$membership %in% idx])
plot(big)
library(tm)
# example...
# first exclude the two malformed records:
cond <- !(plookup$Id %in% p403[!(p403 %in% p401)])
plookup_sub <- plookup[cond,]
# define corpus
corp <- plookup_sub$Abstract
corp <- VCorpus(VectorSource(corp))
# Dakota's magic code to remove invalid unicode:
corp <- tm_map(corp, content_transformer(function(x) iconv(iconv(x, "latin1", "ASCII", sub = ""), sub = "")))
# other transformations:
corp <- tm_map(corp, stripWhitespace)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, content_transformer(tolower))
# stopword removal:
corp <- tm_map(corp, removeWords, stopwords("english"))
names(plookup)
corp <- plookup_sub$PaperText
corp <- VCorpus(VectorSource(corp))
# Dakota's magic code to remove invalid unicode:
corp <- tm_map(corp, content_transformer(function(x) iconv(iconv(x, "latin1", "ASCII", sub = ""), sub = "")))
# other transformations:
corp <- tm_map(corp, stripWhitespace)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, content_transformer(tolower))
# stopword removal:
corp <- tm_map(corp, removeWords, stopwords("english"))
library(SnowballC)
corp <- tm_map(corp, stemDocument)
# e.g.
i <- sample(1:length(corp), 1)
cat(corp[[i]]$content)
tdm <-t(DocumentTermMatrix(corp,
control = list(weighting = function(x){
weightTfIdf(x, normalize = FALSE)
})
))
# calculate euclidean/cosine/KL distance between documents:
library(lsa)
library(fields)
#library(entropy)
N <- length(corp)
doc_term_dist_euclid <- rdist(t(as.matrix(tdm)), t(as.matrix(tdm)))
doc_term_dist_cosine <- cosine(as.matrix(tdm))
library(Rtsne)
tsne_euclid <- Rtsne(doc_term_dist_euclid, max_iter=2000, theta=0.2, is_distance=TRUE, verbose=TRUE)
plot(tsne_euclid$Y, cex=0.5)
plot(tsne_euclid$Y, cex=0.5, max_iter=5000)
tsne_euclid <- Rtsne(doc_term_dist_euclid, max_iter=4000, theta=0.1, is_distance=TRUE, verbose=TRUE)
tsne_euclid <- Rtsne(doc_term_dist_euclid, max_iter=4000, theta=0.1, is_distance=TRUE, verbose=TRUE)
tsne_euclid <- Rtsne(doc_term_dist_euclid, max_iter=4000, is_distance=TRUE, verbose=TRUE)
plot(tsne_euclid$Y, cex=0.5)
tsne_euclid <- Rtsne(doc_term_dist_euclid, max_iter=10000, is_distance=TRUE, verbose=TRUE)
plot(tsne_euclid$Y, cex=0.5)
tsne_euclid <- Rtsne(doc_term_dist_euclid, max_iter=50000, is_distance=TRUE, verbose=TRUE)
V(g)
names(plookup_sub)
names(plookup_sub)$Id
plookup_sub$Id
cond
names(plookup_sub)$Id
names(plookup_sub)
plookup_sub$Title
text(tsne_euclid$Y, labels=plookup_sub$Title, cex=0.1)
plot(tsne_euclid$Y, type='n', axes=FALSE, xlab="", ylab="")
text(tsne_euclid$Y, labels=plookup_sub$Title, cex=0.1)
?welch.test
?t.test
plot(extra ~ group, data = sleep)
group
sleep
n = 398452908570284
log(n,10)
k <- log(2,10)
k
log(n,10)/k
log(n,2)
as.character(n)
strsplit(as.character(n), "")
length(strsplit(as.character(n), ""))
length(strsplit(as.character(n), "")[[1]])
length(strsplit(as.character(n), "")[[1]])/k
floor( length(strsplit(as.character(n), "")[[1]])/k )
f <- function(n)floor( length(strsplit(as.character(n), "")[[1]])/k )
f(8)
f(16)
f(64)
f(32)
idx <- 1:1000
x <- log(idx, 2)
y <- sapply(idx, f)
plt(x,y)
plot(x,y)
x <- floor(log(idx, 2))
plot(x,y)
idx <- 1:10000
x <- floor(log(idx, 2))
y <- sapply(idx, f)
plot(x,y)
abline(0,1)
x <- ceiling(log(idx, 2))
plot(x,y)
abline(0,1)
setwd("~/Blog/aws/_INPROGRESS_201605lanl-rnn/LANL-model/pig")
icmp <- read.csv(pipe("cat icmp_edges.txt | tr -d '()' | cut -d',' -f1,2"))
library(igraph)
icmp
g <- graph.edgelist(as.matrix(icmp))
V(g)$size <- log(1 + degree(g))
V(g)$color <- 'white'
V(g)$label.cex <- 0.05*V(g)$size
E(g)$arrow.mode <- '-'
lout <- layout.fruchterman.reingold(g)
plot(g, layout=lout)
cc <- clusters(g)
str(cc)
cc$no
cc$csize
?induced.subgraph
giant <- induced.subgraph(g, V(g)[cc$membership==1])
V(giant)$size <- log(1 + degree(giant))
V(giant)$color <- 'white'
V(giant)$label.cex <- 0.05*V(giant)$size
E(giant)$arrow.mode <- '-'
lout <- layout.fruchterman.reingold(giant)
plot(giant, layout=lout)
V(giant)$size <- log(1 + degree(giant))
V(giant)$color <- 'red'
V(giant)$label.cex <- 0.05*V(giant)$size
E(giant)$arrow.mode <- '-'
plot(giant)
V(giant)$size <- log(1 + degree(giant))
V(giant)$color <- 'red'
V(giant)$label <- ""
#V(giant)$label.cex <- 0.05*V(giant)$size
E(giant)$arrow.mode <- '-'
plot(giant)
